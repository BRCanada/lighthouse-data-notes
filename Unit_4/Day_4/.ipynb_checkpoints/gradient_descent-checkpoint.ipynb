{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "In this notebook we will implement gradient descent for a single neuron. To simplify this as much as possible we will assume we have no bias `b = 0` and no activation function. The image below shows the neuron we will be coding and the relevant equations needed to update the weight using gradient descent. \n",
    "\n",
    "<img src=\"Gradient Descent Notebook-1.jpg\" width=600 align=\"center\">\n",
    "\n",
    "The recipe we are following is:\n",
    "1. make a prediction for our single sample\n",
    "2. calculate the loss for this single sample\n",
    "3. calculate the value of the derivative for this single sample\n",
    "4. use the derivative and the learning rate to update the value of the weight\n",
    "5. repeat until the loss is a minimum (that is, stops decreasing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent: A single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 4 # correct answer\n",
    "x = 2 # we have one sample and one feature \n",
    "w = 0.1 # initial value for our weight\n",
    "\n",
    "lr = 0.0127 # the learning rate\n",
    "\n",
    "n_iterations = 100 # number of iterations of gradient descent, that is, number of times we update w\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    y_hat = w * x # make prediction using current value for w\n",
    "    L = (y - y_hat)**2  # calculate the loss (our loss function = mean squared error)\n",
    "    dL_dw = -2 * (y - y_hat)*x   # calculate derivative needed to update the weight (see image above) \n",
    "    w = w - lr*dL_dw # update the weight\n",
    "    # the code below allows you to print out answers after every x iterations; adjust to suit your needs\n",
    "    if (i%1 == 0):\n",
    "        print(f\"iteration: {i} weight: {w:.4f}  prediction: {y_hat:.4f}  Loss: {L:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent: Multiple samples\n",
    "\n",
    "We will now repeat what we did above but now our training data will have 5 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 2, 3, 3, 4]  # 5 samples with 1 feature value each\n",
    "Y = [2, 1, 4, 2, 5] # correct answers for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data so we can see what we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the plot above, convince yourself that for our neuron we will not be able to make `L = 0` as we could with the single sample. That is, there is no line we can draw that will pass through all the points (that is how the loss would equal to 0). \n",
    "\n",
    "In the following code we will apply stochastic gradient descent (update weight after each sample in the trainging data) to the training data of 5 samples we created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.1 # initial value for our weight\n",
    "lr = 0.0013\n",
    "\n",
    "n_epochs = 100\n",
    "epoch_counter = 0\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    epoch_counter = epoch_counter + 1 # use this to keep track of how many times we have gone through entire training data\n",
    "    Total_L = 0 # set the total_loss (sum of losses from each sample in training set) to 0 at start of each epoch\n",
    "    for x, y in zip(X, Y): # iterate through the training data one sample at a time\n",
    "        y_hat = w * x # make prediction for current sample\n",
    "        L = (y - y_hat)**2  # calculate loss for current sample\n",
    "        Total_L = Total_L + L # add the loss for current sample to total loss\n",
    "        dL_dw = -2 * (y - y_hat)*x   # calculate derivative for current sample\n",
    "        w = w - lr*dL_dw # update the weight using the derivative and loss for current sample\n",
    "    print(f\"Epoch {epoch_counter} weight: {w:.8f}  Total Loss: {Total_L:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with scikit-learn\n",
    "\n",
    "Our neuron is really just performing linear regression since we have no activation function. So, let's perform linear regression on the data and see what value scikit-learn gives us for `w`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X_arr = np.array(X).reshape(-1, 1)\n",
    "Y_arr = np.array(Y) \n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_arr, Y_arr)\n",
    "\n",
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Batch Gradient Descent\n",
    "\n",
    "We will now implement batch gradient descent where we will only make an update to the weight after we have processed all of the samples. The recipe is as follows: \n",
    "1. make predictions for all samples in our training data\n",
    "2. calculate the average loss for all samples in our training data\n",
    "3. calculate the average value of the derivative for all samples in our training data\n",
    "4. update the value of the weight\n",
    "5. repeat until the loss is a minimum (that is, stops decreasing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([1, 2, 3, 3, 4]).reshape(-1, 1) \n",
    "Y_new = np.array([2, 1, 4, 2, 5]).reshape(-1, 1) \n",
    "\n",
    "w = 0.1 \n",
    "lr = 0.013 \n",
    "\n",
    "n_epochs = 150\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    Y_hat = w*X_new # make predictions for all samples (remember X_new is an array with 5 samples)\n",
    "    sample_losses = (Y_new - Y_hat)**2 # calculate the loss for all 5 samples (result is an array with 5 values)\n",
    "    avg_loss = sample_losses.mean() # calculate the average loss for the training data\n",
    "    dLoss_dw = (-2 * np.multiply((Y_new - Y_hat), X_new)).mean() # calculate the average of the derivatives\n",
    "    w = w - lr*dLoss_dw # update the weight using the average value of the derivative\n",
    "    print(f\"Epoch {i + 1} weight: {w:.4f}  AVG Loss: {avg_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
