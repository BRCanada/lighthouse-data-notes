{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32de6c4d-cca1-4cab-b42d-7754134f70f0",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "The purpose of this is to identify data needed from a data set and parsing the data down to those criteria while retaining data integrity. From a mathematic standpoint, **DR** can be seen as reducing a 3D array to a 2D array.\n",
    "<br>\n",
    "<br> PCA is an industry standard module used for dimensionality (i.e. feature) reduction.\n",
    "<br> LCA will also be covered\n",
    "<br>\n",
    "<br>\n",
    "Feature selection will be discussed:\n",
    "* Filter methods\n",
    "* Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4806c2b-28fa-4e23-aa3d-67a8b884ce6c",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "Big data leads to high-dimensional data, big data includes structured data, images, audio, language/text, and sensors/recordings.\n",
    "\n",
    "* Images: each image has an x and y axis, and each pixel has a plethora of values from hue data to black and white contrast data.\n",
    "* Text: All text found on the internet is a form of list data \n",
    "* Structured Data: Data sets and tables are essentially pandas waiting to be explored.\n",
    "* Audio: All audio has a bitrate and binary data that can be studied or iterated over.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d700d65-4c0d-41cd-aad4-32af69aa2bce",
   "metadata": {},
   "source": [
    "Why reduce dimensionality (number of features)?\n",
    "* Visualization\n",
    "* Improved model performance\n",
    "* Lower computational complexity\n",
    "* DR as unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58123d-ac65-4fba-961e-2d827d8287fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualization\n",
    "The human visual system is the most powerful perceptual system in the known universe... But it only works in up to 3 dimensions.\n",
    "\n",
    "#### Improved Model Performance\n",
    "* Denoising: Reduces clutter in data clusters\n",
    "* Disentangled (uncorrelated) features: Correlated features can turn up where they are so similar it is redundant to have both. Correlated features can aversely effect machine learning models.\n",
    "* Fewer Model Parameters: \\<we will cover this in the future>\n",
    "* Less *overfitting*: Overfitting occurs when the model too accurately fits and performs on a training set, but it performs poorly on a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e375e-855d-46ea-89c9-8320de4bf065",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Lower computational complexity\n",
    "* Dimensionality reduction as compression (less memory load)\n",
    "* Fewer computations (smaller matrix multiplication)\n",
    "\n",
    "##### Dimensionality reduction as unsupervised learning\n",
    "* high-dimensional data often exists on a *low-dimensional* manifold.\n",
    "* By reducing the dimensionality, we might learn the true underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116606a8-d6ed-45c2-ad22-18d9a4a018cb",
   "metadata": {},
   "source": [
    "### Principle Component Analysis (PCA)\n",
    "**PCA Intuition**: \n",
    "<br> The coordinate system data is projected onto has chosen features representing perpendicular x and y axes. Data based on these two features will be plotted as a point on that system. regardless of the orientation of the axes, the point will always be projected in the same original position, however the new axes will provide a different data coordinate for the point. The only time we would want to adjust the axes is when we can identify a 'trend line' and superimpose a perpendicular system with the x-axis being the trend line.\n",
    "<br>\n",
    "<br>The trend line x-axis is referred to as the *1st Principle Component* or PC1. IF there is enough variance for the y axis to plot points effectively, that axis is referred to as the *2nd Principle Component* or PC2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffacb83-3cb8-4050-9dd5-9aa0e48d0b2b",
   "metadata": {},
   "source": [
    "#### Projecting data onto the PCs\n",
    "Project each data point(vector) onto each PC(vector). This projection is done using a **dot product**. We can express the projection of multiple datapoints onto multiple PCs using a matrix multiplication.\n",
    "\n",
    "#### Picking the number of PCs\n",
    "* One strategy: number of PCs up to a certain % of total variance explained. $$ Explained\\,Variance\\,Ratio\\,X\\,Principle\\,Components $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01bf96f-91d6-466a-a58f-ca93eda5404f",
   "metadata": {},
   "source": [
    "### Scaling DF before PCA\n",
    "* PCA finds dimensions with high variance\n",
    "* If some columns of your data have much higher variance, they will dominate their PCs\n",
    "* The variance of these columns is often arbitrary (e.g. mm vs. m units)\n",
    "* Assume each column is equally important by applying StandardScaler\n",
    "\n",
    "#### What do the PCs represent\n",
    "* Size *d* vectors spanning original space\n",
    "* When a data point is projected onto one, gives one number.\n",
    "    * Together, these numbers preserve as much information as possible.\n",
    "    \n",
    "### Linear Discriminant Analysis (LDA)\n",
    "* Like PCA, LDA reduces dimensionality\n",
    "* LDA projections:\n",
    "    * Minimize intra-class variance\n",
    "    * Maximize inter-class variance\n",
    "* Supervised (LDA) vs. Unsupervised(PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da4767-6565-4636-9914-85a7029e53f2",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "* Dimensionality Reduction: Creates new features that are functions of the original ones (for PCA, linear combinations of the original ones).\n",
    "* Feature Selection: Removes redundant features and keeps important ones\n",
    "* Why ever use FS?: Want your resulting feature set to still be interpretable (i.e. not creating new, potentially hard to interpret features)\n",
    "    * Means feature selection is mostly used when features are already interpretable (I.e. structured data like tables with column names)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60e5e1-d15a-4952-89e5-af765b309009",
   "metadata": {},
   "source": [
    "### Filter Methods\n",
    "* Measure relevance of feature correlation with dependent variable (target).\n",
    "* If feature is correlated with target, keep. Otherwise, discard.\n",
    "* Applied **before** training ML model\n",
    "* Advantages:\n",
    "    * Fast, no training involved\n",
    "* Disadvantages:\n",
    "    * Ignores feature combinations\n",
    "    * Keeps redundant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d63e0-01bd-4832-aabd-41822780213e",
   "metadata": {},
   "source": [
    "### Wrapper Methods\n",
    "* Train ML model with different subsets of feature\n",
    "* If feature improves performance, add/keep it. Otherwise, ignore/remove it.\n",
    "* Applied **during** training ML model\n",
    "* Advantages:\n",
    "    * Evaluates features in context of others\n",
    "    * Performanve-driven\n",
    "* Disadvantages: \n",
    "    * Slow, retrain model several times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b19bc3-672a-4bd2-9f6b-bef2af19ee9b",
   "metadata": {},
   "source": [
    "#### Forward Selection Method\n",
    "**1.** *SelectedFeatures = []*\n",
    "<br>**2.** Find *F* in (AllFeatures - SelectedFeatures) that, if added to SelectedFeatures, best improves model performance\n",
    "<br>**3.** If adding *F* improved performance more than some threshold, permanently add it to SelectedFeatures and go back to (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a1be9-d967-466a-9d7d-85d5408d8ca7",
   "metadata": {},
   "source": [
    "#### Backward Elimination Method\n",
    "**1.** *SelectedFeatures = AllFeatures*\n",
    "<br>**2.** Find *F* in SelectedFeatures that, if removed from SelectedFeatures, decreases model performance the least\n",
    "<br>**3.** If removing *F* decreased performance less than some threshold, permanently remove it from SelectedFeatures and go back to (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2f416-0557-4f10-b7ea-4f9432cedc8d",
   "metadata": {},
   "source": [
    "## PCA DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fcd7c0-9ed2-48c3-b2eb-6a1ef202a26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
