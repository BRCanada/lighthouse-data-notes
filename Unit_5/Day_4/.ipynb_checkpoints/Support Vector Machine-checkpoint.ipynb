{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66097714-c3e1-44c2-98a0-d5e9562c6605",
   "metadata": {},
   "source": [
    "# Support Vector Machines in Python\n",
    "\n",
    "The SVM algorithm is implemented in practice using a kernel. A kernel transforms an input dataspace into the required form. The kernel takes a low dimensional input space and transforms it into a higher dimensional space. In other words, you can say that it converts nonseperable problems into seperable ones by adding extra dimensions to the datapoints.\n",
    "<br>\n",
    "It is most useful in non-linear classification problems, and helps you build a more accurate classifier.\n",
    "<br>\n",
    "<br>\n",
    "##### Linear Kernel\n",
    "A linear kernel can be used as a normal *dot product* of any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values:\n",
    "\n",
    "$$ K(x, xi) = sum(x*xi) $$\n",
    "\n",
    "\n",
    "##### Polynomial Kernel\n",
    "A more generalized form of the linear kernel. The polynomial kernel can distinguish curved or nonlinear input space:\n",
    "\n",
    "$$ K(x, xi) = 1 + sum(x*xi)^{d} $$\n",
    "\n",
    "Where d is the degree of the polynomial. d=1 is similar to linear transformation. The degree needs to be manually specified in the learning algorithm.\n",
    "\n",
    "##### Radial Basis Function Kernel\n",
    "The radial basis function kernel is a popular kernel function commonly used in support vector machine classification. RBF can map an input space in infinite dimensions.\n",
    "\n",
    "$$ K(x, xi) = \\exp (-\\gamma * sum(x-xi^{2})) $$\n",
    "\n",
    "Here, gamma is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. Gamma=0.1 is considered to be a good default. The value of gamma needs to be specified manually in the learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e99e95-3a16-4258-a08e-51fdb6421ad8",
   "metadata": {},
   "source": [
    "### Classifier Building in SciKit Learn\n",
    "In this tutorial, we will use the sklearn cancer dataset, which is a famous multi-class classification problem. This dataset is computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "<br>\n",
    "<br>\n",
    "The dataset comprises 30 features: 29 observations and 1 target (cancer type). \n",
    "<br>\n",
    "<br>\n",
    "This data has two types of cancer classes: malignant (harmful) and benign (not harmful). Here, you can build a model to classify the type of cancer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52cd32fa-1a64-4967-b487-dee51e76c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data and libraries\n",
    "from sklearn import datasets\n",
    "\n",
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d445fc94-7f89-404c-bb3b-0a2d3b904636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Labels:  ['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "#EXPLORE THE DATA\n",
    "# print the names of the 13 features\n",
    "print(\"Features: \", cancer.feature_names)\n",
    "\n",
    "# print the label type of cancer('malignant' 'benign')\n",
    "print(\"Labels: \", cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41db7d4-99e9-46b5-b9d8-2c813de70e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print data(feature)shape\n",
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ab4e2c-0c97-4ef5-9c2c-88331f9b8122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      "  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      "  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      "  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      "  4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
      "  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
      "  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
      "  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
      "  2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 1.203e+03 1.096e-01 1.599e-01 1.974e-01\n",
      "  1.279e-01 2.069e-01 5.999e-02 7.456e-01 7.869e-01 4.585e+00 9.403e+01\n",
      "  6.150e-03 4.006e-02 3.832e-02 2.058e-02 2.250e-02 4.571e-03 2.357e+01\n",
      "  2.553e+01 1.525e+02 1.709e+03 1.444e-01 4.245e-01 4.504e-01 2.430e-01\n",
      "  3.613e-01 8.758e-02]\n",
      " [1.142e+01 2.038e+01 7.758e+01 3.861e+02 1.425e-01 2.839e-01 2.414e-01\n",
      "  1.052e-01 2.597e-01 9.744e-02 4.956e-01 1.156e+00 3.445e+00 2.723e+01\n",
      "  9.110e-03 7.458e-02 5.661e-02 1.867e-02 5.963e-02 9.208e-03 1.491e+01\n",
      "  2.650e+01 9.887e+01 5.677e+02 2.098e-01 8.663e-01 6.869e-01 2.575e-01\n",
      "  6.638e-01 1.730e-01]\n",
      " [2.029e+01 1.434e+01 1.351e+02 1.297e+03 1.003e-01 1.328e-01 1.980e-01\n",
      "  1.043e-01 1.809e-01 5.883e-02 7.572e-01 7.813e-01 5.438e+00 9.444e+01\n",
      "  1.149e-02 2.461e-02 5.688e-02 1.885e-02 1.756e-02 5.115e-03 2.254e+01\n",
      "  1.667e+01 1.522e+02 1.575e+03 1.374e-01 2.050e-01 4.000e-01 1.625e-01\n",
      "  2.364e-01 7.678e-02]]\n"
     ]
    }
   ],
   "source": [
    "# print the cancer data features (top 5 records)\n",
    "print(cancer.data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eaa26f-073e-4d62-a9cf-cfc644a8983d",
   "metadata": {},
   "source": [
    "Let's look at the target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6070654a-189c-44df-86b8-27d7861c16de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# print the cancer labels (0:malignant, 1:benign)\n",
    "print(cancer.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10df41-2c4f-498b-8939-4bf066e02379",
   "metadata": {},
   "source": [
    "#### Splitting Data\n",
    "To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n",
    "Let's use train_test_split() with 3 parameters: features, target, and test_set size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706c1a4d-0044-4c06-8492-f7c302231341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f51ea-bc6b-452e-8877-6df58181de1b",
   "metadata": {},
   "source": [
    "#### Generating Model\n",
    "Let's build a support vector machine model. First, Import the SVM module and create a support vector classifier object by passing argument kernal as the linear kernel in SVC() function.\n",
    "\n",
    "Then, fit your model on train set using fit() and perform prediction on the test set using predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0dd8544-4c38-47bb-824c-3788079e2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675ef7e-d853-4df7-9236-6f6dc8d1bb37",
   "metadata": {},
   "source": [
    "#### Evaluating the Model\n",
    "Let's estimate how accurately the classifier or model can predict the breast cancer of patients.\n",
    "<br>\n",
    "Accuracy can be computed by comparing actual test set values and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58c39b5-85c0-4ed2-8caf-159ae32a5fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fa10c-ddbe-4660-b3f7-24c6cba58acf",
   "metadata": {},
   "source": [
    "Well, you got a classification rate of 96.49%, considered as very good accuracy.\n",
    "\n",
    "For further evaluation, you can also check precision and recall of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d20bff8-20b2-43ba-a84c-d9289d3c9db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9811320754716981\n",
      "Recall: 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03bdab1-69f0-4ab9-ae6c-024fc75d669e",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters\n",
    "* **Kernel**: The main function of the kernel is to transform the given dataset input data into the required form. There are various types of functions such as linear, polynomial, and radial basis function (RBF). Polynomial and RBF are useful for non-linear hyperplane. Polynomial and RBF kernels compute the separation line in the higher dimension. In some of the applications, it is suggested to use a more complex kernel to separate the classes that are curved or nonlinear. This transformation can lead to more accurate classifiers.\n",
    "\n",
    "* **Regularization**: Regularization parameter in python's Scikit-learn C parameter used to maintain regularization. Here C is the penalty parameter, which represents misclassification or error term. The misclassification or error term tells the SVM optimization how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term. A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane.\n",
    "\n",
    "* **Gamma**: A lower value of Gamma will loosely fit the training dataset, whereas a higher value of gamma will exactly fit the training dataset, which causes over-fitting. In other words, you can say a low value of gamma considers only nearby points in calculating the separation line, while the a value of gamma considers all the data points in the calculation of the separation line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea4c1f-c5d1-49bd-b888-d97007dae9fd",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "SVM Classifiers offer good accuracy and perform faster prediction compared to Naïve Bayes algorithm. They also use less memory because they use a subset of training points in the decision phase. SVM works well with a clear margin of separation and with high dimensional space.\n",
    "\n",
    "### Disadvantages\n",
    "SVM is not suitable for large datasets because of its high training time and it also takes more time in training compared to Naïve Bayes. It works poorly with overlapping classes and is also sensitive to the type of kernel used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dce9e4-1f0e-42b0-99ea-d498cf23abc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
