{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83620eb-c2dc-4571-a85c-0bf2bb7fe56c",
   "metadata": {},
   "source": [
    "# Decision Trees in Python\n",
    "### Decision Tree Algorithm\n",
    "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The top note is referred to as the *Root* node, and it learns to partition on the basis of the attribute value. \n",
    "<br>\n",
    "<br> Partitions are made in a recursive manner. And this structure helps with decision making. It's closest relative is a generic flow chart, which essentially mimics human-level thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5403389-c5fe-4fe7-83a1-752a5e1b2465",
   "metadata": {},
   "source": [
    "### How Does it Work?\n",
    "Here is the basic idea of a decision tree's parts:\n",
    "1. Select the best attribute using Attribute Selection Measures to split the data records.\n",
    "2. Make that attribute a decision node and breaks the dataset into smaller subsets.\n",
    "3. Starts tree building by repeating this proceess recursively for each child until one of the conditions will match:\n",
    "    * All the tuples belong to the same attribute value\n",
    "    * There are no more remaining attributes\n",
    "    * There are no more instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362b560-8090-4566-b5b7-3bb1b51a1084",
   "metadata": {},
   "source": [
    "### Attribute Selection Measures\n",
    "This is a heuristic for selecting the splitting criterion that partition data in the best possible manner. It is also known as 'splitting rules' because it helps us to determine breakpoints for tuples on a given node. ASM provides a rank to each feature(or attribute) by explaining the given dataset. The attribute with the best score (i.e. lowest Gini value) will be selected as a splitting attribute. Popular measures are Information Gain, Gain Ratio and Gini Index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc419a-8d53-4517-aa96-e6082905fbed",
   "metadata": {},
   "source": [
    "#### Information Gain\n",
    "Shannon coined the concept of entropy, which measures the impurity of the input set. In physics and mathematics, entropy referred as the randomness or the impurity in the system. In information theory, it refers to the impurity in a group of examples. Information Gain is the decrease in entropy. IG computes the difference between entropy before the split, and the average entropy after the split based on given attribute values. The **Iterative Dichotomiser (ID3** decision tree algorithm uses the IG method.\n",
    "\n",
    "$$ Gain(A) = Info(D) - Info_{A}(D) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f48266-1df2-4d62-8ab3-67d5a6f8400c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gain Ratio\n",
    "Information Gain is biased for the attributes that have many outcomes. This means IG prefers the attribute with a *large number* of distinct values. For instance, consider if an attribute with a unique identifier, such as customer_ID, has *no* info due to pure partition. This maximizes information gain and creates useless partitioning.\n",
    "<br>\n",
    "<br>\n",
    "**C4.5**, an improvement of the **ID3** algorithm, uses an extension to information gain known as the *Gain Ratio*. Gain Ratio handles the issue of bias by normalizing the information gain using split info.\n",
    "\n",
    "$$ GainRatio(A) = \\frac{Gain(A)}{SplitInfo_{A}(D)} $$\n",
    "\n",
    "The attribute with the highest gain ratio is chosen as the splitting attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96493d6d-dc64-4bed-a903-74d991e06cf9",
   "metadata": {},
   "source": [
    "#### Gini Index\n",
    "Another decision tree algorithm, **CART (Classification and Regression Tree)**, uses the Gini method to create split points. The Gini Index considers a binary split for each attribute. You can compute a weighted sum of the impurity if each partition. If a binary split on attribute *A* partitions data *D* into *D1* and D2*, the Gini Index of D is:\n",
    "<br> \n",
    "\n",
    "$$ Gini_{A}(D) = \\frac{D1}{D}Gini(D_{1})+\\frac{D2}{D} Gini(D_{2}) $$\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c206d29b-cf64-427c-aa6a-3a985bc5acc5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44ddae6e-1c51-483a-b6c7-496b9fd7015a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb799633-000f-443d-b5a7-3b5d8634c550",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
