{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbad4e2-0f7a-4806-bb9d-487ddf4ee659",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "## Regression Models Evaluation\n",
    "To demonstrate evaluation, we won't be using any regression model, but will generate original values and predictions from the model by **Numpy** instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee36bf3-63ed-4424-9ef3-abc5e23152a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e79c49a-f53f-4403-b8a9-f980c1112389",
   "metadata": {},
   "source": [
    "WE will generate 10 observations (**y_true**) and 10 predictions (**y_pred**) from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7d9f3e-989f-4003-8de2-5429df79f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 'ground truth'\n",
    "y_true = np.random.normal(0,1,10)\n",
    "\n",
    "# generate random errors\n",
    "errors = np.random.normal(0, 0.02, 10)\n",
    "\n",
    "# simulate predictions\n",
    "y_pred = y_true + errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96fd2e-fbed-47ce-bcba-0d10373beddc",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)/Root Means Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a38c19-c5dc-4824-a8c5-e5e80621356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00023585438640638011\n"
     ]
    }
   ],
   "source": [
    "# import MSE from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# compute MSE\n",
    "MSE = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# print MSE\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fa0ce-9415-404f-98f7-c6a4e79357fc",
   "metadata": {},
   "source": [
    "All regression evaluations from sklearn.metrics take two mandatory arrays as parameters. The first is an array with ground truth values (in our case ```y_true``` variable) and the second is our prediction (in our case ```y_pred``` variable).\n",
    "<br>\n",
    "<br>\n",
    "To get RMSE from MSE we have two options: the first option is to compute the square root from MSE by **Numpy** and the second option is to use the ```squared=False``` option in a function. Let's try both options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36391731-12b9-4a5b-a218-d021142f74c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01535755144566933\n",
      "0.01535755144566933\n"
     ]
    }
   ],
   "source": [
    "# RMSE by Numpy\n",
    "RMSE = np.sqrt(MSE)\n",
    "print(RMSE)\n",
    "\n",
    "# RMSE by sklearn\n",
    "RMSE = mean_squared_error(y_true,y_pred,squared=False)\n",
    "print(RMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba02edff-0364-4fd2-b2c2-105b98e89ac7",
   "metadata": {},
   "source": [
    "### Classification Models Evaluation\n",
    "We will use the same principle as in regression model evaluation and use synthetic data. With the only difference that we will need predicted probabilities instead of predicted labels (predicted values in regression). The important thing to mention is that we are simulating the behavior of a binary classifier. It means that the predicted class is only positive (returns 1) or negative(returns 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec86927-00c5-431d-a6c2-fe6e6a6b75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth\n",
    "y_true = [1,1,0,1,0,0,1,0,0,1]\n",
    "\n",
    "# simulate probabilites of positive class\n",
    "y_proba = [0.9,0.7,0.2,0.99,0.7,0.1,0.5,0.2,0.4,0.6]\n",
    "\n",
    "# set the threshold to predict positive class\n",
    "thres = 0.5\n",
    "\n",
    "# class predictions\n",
    "y_pred = [int(value > thres) for value in y_proba]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa44b51-9915-419e-9eb4-93c14701147c",
   "metadata": {},
   "source": [
    "```y_proba``` we have the probabilities of the observations from the positive class. We also set the ```threshold``` value. All observations with probabilities above this threshold are assigned to the positive class and stored in the ```y_pred``` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1d80b-0cc8-4bc3-a4e3-2b0659212730",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246cef61-9153-4b2e-ab24-b2a6eafc1962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# import accuracy_score from sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(y_true,y_pred)\n",
    "\n",
    "# print accuracy\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e919b-2e53-4b28-8273-4f7d2da043f5",
   "metadata": {},
   "source": [
    "#### F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f110a76d-1eef-441c-912b-1c522236dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8000000000000002\n"
     ]
    }
   ],
   "source": [
    "# import f1_score from sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# compute F1-score\n",
    "f1_score = f1_score(y_true,y_pred)\n",
    "\n",
    "# print F1-score\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810897a6-d204-4ada-ab11-a2765d75d912",
   "metadata": {},
   "source": [
    "#### AUC-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dab67b5-deec-4fbd-a04f-8cc87ef53e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# import roc_auc_score from sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# compute AUC-score\n",
    "auc = roc_auc_score(y_true,y_proba)\n",
    "\n",
    "# print AUC-score\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158d3b0-7bb4-430b-9715-af0c12424730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
